{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!pip install nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjPjpq7bqBB4",
        "outputId": "55a5429c-2246-48e3-c57c-c75e1bdcaaad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://cli.github.com/packages stable InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,426 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,821 kB]\n",
            "Fetched 12.3 MB in 3s (4,251 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Requirement already satisfied: nvcc4jupyter in /usr/local/lib/python3.12/dist-packages (1.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3648409b",
        "outputId": "cf2ffc81-fac4-4d82-c945-3bf0420e87d0"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be441b46",
        "outputId": "2a0f75df-b002-433f-84ae-f66cac80df9d"
      },
      "source": [
        "%load_ext nvcc4jupyter"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmpi5a8kk1k\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üßæ **Report ‚Äî CUDA Hello World (Problem 1)**\n",
        "\n",
        "**Name:** Uttkarsh Malviya\n",
        "\n",
        "**Roll Number:** IIT2022061\n",
        "\n",
        "---\n",
        "\n",
        "**(a) Experimental Setup:**\n",
        "\n",
        "* Platform: Google Colab (GPU runtime enabled)\n",
        "* GPU: NVIDIA Tesla T4 (Compute Capability 7.5)\n",
        "* Compiler: *nvcc* (CUDA Toolkit preinstalled on Colab)\n",
        "\n",
        "\n",
        "**(b) Description:**\n",
        "A simple CUDA ‚ÄúHello World‚Äù program that prints each thread‚Äôs block ID, thread ID,\n",
        "and its computed global thread ID.\n",
        "\n",
        "---\n",
        "\n",
        "**(c) Brief Summary of Results:**\n",
        "Each GPU thread successfully printed its unique identifiers.\n",
        "Output order varied slightly between blocks because of parallel execution.\n",
        "\n",
        "---\n",
        "\n",
        "**(d) Remarks / Observations:**\n",
        "\n",
        "* Kernel configuration: **3 blocks √ó 4 threads per block**\n",
        "* Total threads launched: **12**\n",
        "* Execution time: negligible (a few microseconds)\n",
        "* Output order interleaved due to concurrent thread execution\n",
        "* Demonstrates correct mapping between **blockIdx**, **threadIdx**, and global index\n"
      ],
      "metadata": {
        "id": "UhkWRZJgXT3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hello_world_cuda.cu\n",
        "#include <cstdio>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void helloFromGPU()\n",
        "{\n",
        "    int tid  = threadIdx.x;                 // Thread ID within block\n",
        "    int bid  = blockIdx.x;                  // Block ID\n",
        "    int gtid = bid * blockDim.x + tid;      // Global thread ID (1D)\n",
        "    printf(\"Hello from Block %d, Thread %d, Global %d\\n\", bid, tid, gtid);\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    int threadsPerBlock = 4;\n",
        "    int numBlocks = 3;\n",
        "\n",
        "    printf(\"Launching kernel with %d blocks √ó %d threads per block\\n\\n\",\n",
        "           numBlocks, threadsPerBlock);\n",
        "\n",
        "    helloFromGPU<<<numBlocks, threadsPerBlock>>>();\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    printf(\"\\nKernel execution complete.\\n\");\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C1BcgxVZN80",
        "outputId": "ba5509ea-23e6-4415-c200-b5a9a48040f8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting hello_world_cuda.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "./hello_world_cuda\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78Q6tMeWZuky",
        "outputId": "7a331b77-88da-4812-dad9-696eebef27af"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching kernel with 3 blocks √ó 4 threads per block\n",
            "\n",
            "Hello from Block 2, Thread 0, Global 8\n",
            "Hello from Block 2, Thread 1, Global 9\n",
            "Hello from Block 2, Thread 2, Global 10\n",
            "Hello from Block 2, Thread 3, Global 11\n",
            "Hello from Block 0, Thread 0, Global 0\n",
            "Hello from Block 0, Thread 1, Global 1\n",
            "Hello from Block 0, Thread 2, Global 2\n",
            "Hello from Block 0, Thread 3, Global 3\n",
            "Hello from Block 1, Thread 0, Global 4\n",
            "Hello from Block 1, Thread 1, Global 5\n",
            "Hello from Block 1, Thread 2, Global 6\n",
            "Hello from Block 1, Thread 3, Global 7\n",
            "\n",
            "Kernel execution complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üßæ **Report ‚Äî Problem 2: Vector Addition (CUDA)  (Problem 2)**\n",
        "\n",
        "**Name:** Uttkarsh Malviya\n",
        "\n",
        "**Roll Number:** IIT2022061\n",
        "\n",
        "---\n",
        "\n",
        "**(a) Experimental Setup:**\n",
        "\n",
        "* **Platform:** Google Colab (GPU runtime enabled)\n",
        "* **GPU:** NVIDIA Tesla T4 (Compute Capability 7.5)\n",
        "* **Compiler:** `nvcc` (CUDA Toolkit 12.x preinstalled on Colab)\n",
        "* **Compilation Command:**\n",
        "\n",
        "  ```\n",
        "  nvcc -arch=sm_75 vec_add.cu -o vec_add\n",
        "  ```\n",
        "* **Execution Command:**\n",
        "\n",
        "  ```\n",
        "  ./vec_add\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "**(b) Description:**\n",
        "This program performs element-wise addition of two vectors using CUDA.\n",
        "Each GPU thread adds one element from vectors **A** and **B** and stores the result in **C**.\n",
        "\n",
        "---\n",
        "\n",
        "**(c) Summary of Results:**\n",
        "The kernel executed successfully, producing correct output for all 1 million elements.\n",
        "Verification confirmed that every element matched the expected CPU result.\n",
        "\n",
        "---\n",
        "\n",
        "**(d) Remarks / Observations:**\n",
        "\n",
        "* **Vector size:** 1,048,576 elements (‚âà 1 million)\n",
        "* **Kernel configuration:** 256 threads per block, 3907 blocks\n",
        "* **Total threads launched:** ‚âà 1 million\n",
        "* **Execution time:** Negligible for this size (well under 1 ms)\n",
        "* **Result:** `Vector addition successful!`\n",
        "* Demonstrates correct memory transfer between host ‚Üî device and parallel computation on the GPU.\n",
        "* Output order is deterministic since no device printing occurs.\n"
      ],
      "metadata": {
        "id": "cSX70ircjkWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vec_add.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// CUDA Kernel: performs element-wise vector addition\n",
        "__global__ void vectorAdd(float *A, float *B, float *C, int N)\n",
        "{\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < N)\n",
        "        C[i] = A[i] + B[i];\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    int N = 1 << 20; // 1 million elements\n",
        "    size_t size = N * sizeof(float);\n",
        "\n",
        "    // Allocate host memory\n",
        "    float *hA = (float *)malloc(size);\n",
        "    float *hB = (float *)malloc(size);\n",
        "    float *hC = (float *)malloc(size);\n",
        "\n",
        "    // Initialize host arrays\n",
        "    for (int i = 0; i < N; i++)\n",
        "    {\n",
        "        hA[i] = 1.0f;\n",
        "        hB[i] = 2.0f;\n",
        "    }\n",
        "\n",
        "    // Allocate device memory\n",
        "    float *dA, *dB, *dC;\n",
        "    cudaMalloc((void **)&dA, size);\n",
        "    cudaMalloc((void **)&dB, size);\n",
        "    cudaMalloc((void **)&dC, size);\n",
        "\n",
        "    // Copy input data to device\n",
        "    cudaMemcpy(dA, hA, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dB, hB, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch the kernel\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(dA, dB, dC, N);\n",
        "\n",
        "    // Copy the result back to host\n",
        "    cudaMemcpy(hC, dC, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Verify the result\n",
        "    int errors = 0;\n",
        "    for (int i = 0; i < N; i++)\n",
        "    {\n",
        "        if (fabs(hC[i] - (hA[i] + hB[i])) > 1e-5)\n",
        "        {\n",
        "            errors++;\n",
        "            if (errors < 10)\n",
        "                printf(\"Error at index %d: GPU = %f, Expected = %f\\n\", i, hC[i], hA[i] + hB[i]);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (errors == 0)\n",
        "        printf(\"Vector addition successful!\\\\n\");\n",
        "    else\n",
        "        printf(\"Vector addition failed with %d errors.\\\\n\", errors);\n",
        "\n",
        "    // Free memory\n",
        "    free(hA);\n",
        "    free(hB);\n",
        "    free(hC);\n",
        "    cudaFree(dA);\n",
        "    cudaFree(dB);\n",
        "    cudaFree(dC);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5iiDyGJZvNE",
        "outputId": "d9ec72bf-4690-4cee-fbc7-6830e12ec5bb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vec_add.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "nvcc -arch=sm_75 vec_add.cu -o vec_add\n",
        "./vec_add\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygaYiXZsg3ea",
        "outputId": "31f7637f-2ede-4129-90bc-2adda2903bec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector addition successful!\\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üßæ **Report ‚Äî Problem 3: Scalar Multiplication (CUDA) (Problem 3)**\n",
        "\n",
        "**Name:** Uttkarsh Malviya\n",
        "\n",
        "**Roll Number:** IIT2022061\n",
        "\n",
        "---\n",
        "\n",
        "**(a) Experimental Setup:**\n",
        "\n",
        "* **Platform:** Google Colab (GPU runtime enabled)\n",
        "* **GPU:** NVIDIA Tesla T4 (Compute Capability 7.5)\n",
        "* **Compiler:** `nvcc` (CUDA Toolkit 12.x preinstalled)\n",
        "* **Compilation Command:**\n",
        "\n",
        "  ```\n",
        "  nvcc -arch=sm_75 scalar_mul.cu -o scalar_mul\n",
        "  ```\n",
        "* **Execution Command:**\n",
        "\n",
        "  ```\n",
        "  ./scalar_mul\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "**(b) Description:**\n",
        "This program multiplies every element of an input vector by a scalar value in parallel using CUDA.\n",
        "Each GPU thread handles one element:\n",
        "[\n",
        "A[i] = A[i] times {scalar}\n",
        "]\n",
        "\n",
        "---\n",
        "\n",
        "**(c) Summary of Results:**\n",
        "The GPU kernel executed successfully.\n",
        "Verification confirmed that all elements were multiplied correctly by 5.0.\n",
        "Output message ‚Äî `Scalar multiplication successful!`\n",
        "\n",
        "---\n",
        "\n",
        "**(d) Remarks / Observations:**\n",
        "\n",
        "* **Vector size:** 1,048,576 elements\n",
        "* **Scalar value:** 5.0\n",
        "* **Kernel configuration:** 256 threads per block, 3907 blocks\n",
        "* **Execution time:** negligible (< 1 ms for this size)\n",
        "* Demonstrates one-to-one mapping of GPU threads to data elements.\n",
        "* Validates correct use of global memory access and kernel launch configuration.\n"
      ],
      "metadata": {
        "id": "O_hjEhwMkcLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scalar_mul.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// CUDA kernel: multiply each element of array A by a scalar\n",
        "__global__ void scalarMultiply(float *A, float scalar, int N)\n",
        "{\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < N)\n",
        "        A[i] *= scalar;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    int N = 1 << 20;  // 1 million elements\n",
        "    size_t size = N * sizeof(float);\n",
        "    float scalar = 5.0f;\n",
        "\n",
        "    // Allocate host memory\n",
        "    float *hA = (float *)malloc(size);\n",
        "    for (int i = 0; i < N; i++)\n",
        "        hA[i] = 1.0f;  // initialize all to 1 for easy verification\n",
        "\n",
        "    // Allocate device memory\n",
        "    float *dA;\n",
        "    cudaMalloc((void **)&dA, size);\n",
        "\n",
        "    // Copy input data to device\n",
        "    cudaMemcpy(dA, hA, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch kernel\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    scalarMultiply<<<blocksPerGrid, threadsPerBlock>>>(dA, scalar, N);\n",
        "\n",
        "    // Copy result back to host\n",
        "    cudaMemcpy(hA, dA, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Verify results (print first 10 values)\n",
        "    int errors = 0;\n",
        "    for (int i = 0; i < N; i++)\n",
        "    {\n",
        "        if (fabs(hA[i] - 5.0f) > 1e-5f)\n",
        "        {\n",
        "            if (errors < 10)\n",
        "                printf(\"Error at index %d: GPU=%f, Expected=%f\\n\", i, hA[i], 5.0f);\n",
        "            errors++;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (errors == 0)\n",
        "        printf(\"Scalar multiplication successful!\\n\");\n",
        "    else\n",
        "        printf(\"Scalar multiplication failed with %d errors.\\n\", errors);\n",
        "\n",
        "    // Free memory\n",
        "    free(hA);\n",
        "    cudaFree(dA);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIIn1Ul6g-8G",
        "outputId": "97785dee-8be7-4083-eab3-60644d00b1df"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scalar_mul.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "nvcc -arch=sm_75 scalar_mul.cu -o scalar_mul\n",
        "./scalar_mul\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOI1tUJ_kVE_",
        "outputId": "47f02b7d-7b95-4e45-e396-dfadb796395e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scalar multiplication successful!\n"
          ]
        }
      ]
    }
  ]
}